# Cultural Bias in Machine Intelligence
Journalistic coverage, process and technological approaches to inclusive analytical practices, sociotechnological theory, and recommended readings.

_Please submit pull requests! You are part of the human forces that keep this up-to-date._

## Talk

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQqfx17JaE4TvpgfCqeSen456NBMu8LIvSOJeXNoc-3DrMqq4EDI_h5p3mBPn7J9ECqT5QCZxfhNenN/embed?start=false&loop=false&delayms=3000" frameborder="0" width="480" height="299" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
_Clare Corthell, July 17, 2018_

## Resources

### Case Studies
* [COMPAS and Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) ProPublica 2016
* [“Gaydar” algo and junk science](https://medium.com/@blaisea/do-algorithms-reveal-sexual-orientation-or-just-expose-our-stereotypes-d998fafdf477) Blaise Aguera y Arcas 2017
* [Discrimination in Online Ad Delivery](https://arxiv.org/abs/1301.6822) Latanya Sweeney 2013
* [Reinforcement of Racism in Dating Apps](https://www.npr.org/2018/01/09/575352051/least-desirable-how-racial-discrimination-plays-out-in-online-dating) NPR 2018
* [Amazon Delivery Redlining](https://www.bloomberg.com/graphics/2016-amazon-same-day/) Bloomberg 2016
* [Bias detectives: the researchers striving to make algorithms fair](https://www.nature.com/articles/d41586-018-05469-3) Nature 2018
* [Amazon Employees demand cancellation of facial recognition tool sales](https://gizmodo.com/amazon-workers-demand-jeff-bezos-cancel-face-recognitio-1827037509) 2018
* [The Coded Gaze](https://www.media.mit.edu/people/joyab/overview/) Joy Buolamwini, MIT Media Lab
* [Trump's catch-and-detain policy snares many who call the U.S. home](https://www.reuters.com/investigates/special-report/usa-immigration-court/) -- "ICE modified a tool officers have been using since 2013 when deciding whether an immigrant should be detained or released on bond. The computer-based Risk Classification Assessment uses statistics to determine an immigrant’s flight risk and danger to society. Previously, the tool automatically recommended either “detain” or “release.” Last year, ICE spokesman Bourke said, the agency removed the “release” recommendation, but he noted that ICE personnel can override it.”

### State of the Union
* [AINow 2017 Report](https://ainowinstitute.org/AI_Now_2017_Report.pdf)
* [Responsible AI in Consumer Enterprise Integrate AI](https://www.integrate.ai/responsible-ai-in-consumer-enterprise)

### Think Tanks / Working Groups / Courses
* [AINow](https://ainowinstitute.org/)
* [Data & Society](https://datasociety.net/)
* [Data Justic Lab](https://datajusticelab.org/)
* [FAT* Conference](https://fatconference.org/) (formerly [FATML](http://www.fatml.org/resources/principles-for-accountable-algorithms))
* ICML Workshop: (historically) #Data4Good, Fairness in Machine Learning
* _See also the comprehensive_ [FAT* Index](https://fatconference.org/links.html)
* [CS 294: Fairness in Machine Learning](https://fairmlclass.github.io/) UC Berkeley, Instructor Moritz Hardt

### Talks & Experts
* [The Trouble with Bias / video](https://www.youtube.com/watch?v=fMym_BKWQzk) Kate Crawford 2018
* [TED Machine Intelligence makes morals more important](https://www.ted.com/talks/zeynep_tufekci_machine_intelligence_makes_human_morals_more_important) Zeynep Turfeci
* [Social and Political Questions](https://www.youtube.com/watch?v=a2IT7gWBfaE) Kate Crawford
* [Artificial Intelligence’s White Guy Problem](https://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html?_r=1) Kate Crawford, NYTimes 2016
* [Ethics Will Shape the Customer Experience of AI](https://www.linkedin.com/pulse/ethics-shape-customer-experience-ai-susan-etlinger) Susan Etlinger
* [What is the Problem to Which Fair Machine Learning is the Solution? video](https://www.youtube.com/watch?v=S_AkPi6-r3Y) Solon Barocas 2017
* [NYAI #20: Ethical Algorithms - Bias and Explainability in Machine Learning Systems](https://www.youtube.com/watch?v=BajPM1X9KfQ) Kathryn Hume

### Syntheses / Long Form Reporting
* [Unmasking A.I.'s Bias Problem](http://www.fortune.com/longform/ai-bias-problem/) Fortune 2018

### Legal Context, Ethical Frameworks
* [Big Data's Disparate Impact](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899) Selbst & Barocas

### Recommended Books
* [Weapons of Math Destruction](https://amzn.to/2LpiUQs) Cathy O’Neil
* [Automating Inequality](https://amzn.to/2JBxks3) Virginia Eubanks
* [Frankenstein: Annotated for Scientists, Engineers, and Creators of All Kinds](https://amzn.to/2zUkt4z) Mary Shelley
* [Wind, Sand, and Stars](https://amzn.to/2uxkera) Antoine de Saint Exupery

### Podcasts
* [Cathy O'Neil: Do Algorithms Perpetuate Human Bias?](https://www.npr.org/2018/01/26/580617998/cathy-oneil-do-algorithms-perpetuate-human-bias) NPR January 26, 2018
* [Episode 74: How to Avoid Bias in Your Machine Learning Models with Clare Corthell](https://soundcloud.com/the-impact-podcast/episode-74-how-to-avoid-bias-in-your-machine-learning-models-with-clare-corthell) The Impact Podcast, Georgian Partners 2018
* [Ep 43: Is there Bias in Machine Learning Algorithms? – with guest Dr. Joshua Kroll](http://sparkdialog.com/bias-in-machine-learning-algorithms/) SparkDialog with Elizabeth Fernandez, May 1 2018

### Research: Cultural Bias in Data
* [Quantifying and Reducing Stereotypes in Word Embeddings](https://arxiv.org/pdf/1606.06121.pdf) Bolukbasi et al 2016
* [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/pdf/1607.06520.pdf) Bolukbasi et al 2016

### Papers & Technical Approaches
* [Fairness Through Awareness](https://arxiv.org/abs/1104.3913) Dwork et al 2011
* [Fairness in Machine Learning / NIPS 2017 Tutorial](http://mrtz.org/nips17/#/), Solon Barocas & Moritz Hardt 2017
* [Equality of Opportunity in Supervised Learning](https://arxiv.org/abs/1610.02413) Hardt et al 2016
* [Learning Fair Representations](http://proceedings.mlr.press/v28/zemel13.pdf) Zemel et al 2013

### Researchers & Data People to Follow
* [Kate Crawford](https://twitter.com/katecrawford), Co-founder of AINow Institute
* [Cathy O'Neil](https://mathbabe.org/), Author of [Weapons of Math Destruction](https://amzn.to/2LpiUQs)
* [Moritz Hardt](http://mrtz.org/about.html), Co-founder and co-organizer of FATML
* [Cynthia Dwork](https://www.seas.harvard.edu/directory/dwork), Harvard Gordon McKay Professor of Computer Science, Cryptography Expert and Author of [Differential Privacy](https://link.springer.com/chapter/10.1007/11787006_1) and works on fairness in classification.
* [Kathryn Hume](https://twitter.com/HumeKathryn), Integrate AI Product Strategy and Speaker on history and philosophy of AI
* [Virginia Eubanks](https://virginia-eubanks.com/), Author of [Automating Inequality](https://amzn.to/2JBxks3)
* [Joy Buolamwini](https://www.media.mit.edu/people/joyab/overview/) of MIT Media Lab & Founder of [AI Justice League](https://www.ajlunited.org/)
* [Susan Etlinger](https://www.ted.com/speakers/susan_etlinger), Industry Expert at Altimeter Group
* [Solon Barocas](http://solon.barocas.org/), Assistant Professor in the Department of Information Science at Cornell University & Co-founder of FATML (I suggest following his website's speaking calendar!)
* [Richard Zemel](http://www.cs.toronto.edu/~zemel/inquiry/home.php), Co-Founder and Director of Research, Vector Institute for Artificial Intelligence
* [Timnit Gebru](http://ai.stanford.edu/~tgebru/) Fairness Accountability Transparency and Ethics (FATE) Microsoft Lab, Black in AI
* [Clare Corthell](http://clarecorthell.org), Industry Speaker on building more inclusive digital products

### Contact
Submit a Pull Request! I'm reachable at `clare at luminantdata.com`.
